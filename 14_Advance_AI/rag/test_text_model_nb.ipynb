{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tharhtet/.local/share/virtualenvs/rag-9Lk4wNMW/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  Pipeline,pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_model(device_name):\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        \n",
    "    \n",
    "    )\n",
    "    pipe.to(device_name)\n",
    "\n",
    "    return pipe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = \"\"\"\n",
    "Your name is AI bot and you are a helpful\n",
    "chatbot responsible for teaching Machine Learning and AI to your users.\n",
    "Always respond in markdown.\n",
    "\"\"\"\n",
    "\n",
    "def generate_text(pipe: Pipeline, prompt: str, temperature: float = 0.7) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ] \n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    predictions = pipe(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    ) \n",
    "    output = predictions[0][\"generated_text\"].split(\"</s>\\n<|assistant|>\\n\")[-1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_name :  mps\n"
     ]
    }
   ],
   "source": [
    "device_name  = None\n",
    "if torch.backends.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "else:\n",
    "    device_name = \"cpu\"\n",
    "print(\"device_name : \",device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TextGenerationPipeline' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is tensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mload_text_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m output \u001b[38;5;241m=\u001b[39m generate_text(pipe, prompt)\n\u001b[1;32m      4\u001b[0m output\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mload_text_model\u001b[0;34m(device_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_text_model\u001b[39m(device_name):\n\u001b[1;32m      2\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m     )\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device_name)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pipe\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextGenerationPipeline' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "prompt = \"What is tensorflow\"\n",
    "pipe = load_text_model(device_name) \n",
    "output = generate_text(pipe, prompt)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"It's always great to meet you! I'm delighted that we can work together on a project. My name is [Your Name], and I'm a writer, editor, and proofreader with experience in various industries. Aside from my expertise, I also possess excellent communication skills and have worked for esteemed clients.\\n\\nMy goal is to provide quality content that meets your needs. Please let me know about any specific requirements you may have or the style of writing you prefer. I'm always open to discussing options to suit your project timelines and deadlines.\\n\\nI look forward to working with you on this assignment, and if there are any further details required, please let me know. Thank you for considering my services!\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ths",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
