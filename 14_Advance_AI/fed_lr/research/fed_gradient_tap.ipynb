{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import ndarrays_to_parameters, Context\n",
    "from flwr.server import ServerApp, ServerConfig\n",
    "from flwr.server import ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr.common import Metrics, NDArrays, Scalar\n",
    "from typing import List, Tuple, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]\n",
    "x_train = x_train.reshape(-1, 28 * 28)  # Flatten images\n",
    "x_test = x_test.reshape(-1, 28 * 28)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def exclude_digits(x_data, y_data, excluded_digits):\n",
    "    mask = ~np.isin(y_data, excluded_digits)  # Create a mask for non-excluded digits\n",
    "    x_filtered = x_data[mask]  # Filter input data\n",
    "    y_filtered = y_data[mask]  # Filter labels\n",
    "    return x_filtered, y_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_1 :  {0, 2, 4, 5, 6, 8, 9}\n",
      "y_train_2 :  {0, 1, 3, 4, 6, 7, 9}\n",
      "y_train_3 :  {0, 1, 2, 3, 5, 7, 8}\n"
     ]
    }
   ],
   "source": [
    "x_train_1, y_train_1  = exclude_digits(x_train, y_train, excluded_digits=[1, 3, 7])\n",
    "print(\"y_train_1 : \",set(y_train_1))\n",
    "\n",
    "\n",
    "x_train_2, y_train_2 = exclude_digits(x_train, y_train, excluded_digits=[2, 5, 8])\n",
    "print(\"y_train_2 : \",set(y_train_2))\n",
    "\n",
    "x_train_3, y_train_3 = exclude_digits(x_train, y_train, excluded_digits=[4, 6, 9])\n",
    "print(\"y_train_3 : \",set(y_train_3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sets = [(x_train_1, y_train_1 ),(x_train_2, y_train_2),(x_train_3, y_train_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_1 :  {0, 2, 4, 5, 6, 8, 9}\n",
      "y_test_2 :  {0, 1, 3, 4, 6, 7, 9}\n",
      "y_test_3 :  {0, 1, 2, 3, 5, 7, 8}\n"
     ]
    }
   ],
   "source": [
    "x_test_1, y_test_1  = exclude_digits(x_test, y_test, excluded_digits=[1, 3, 7])\n",
    "print(\"y_test_1 : \",set(y_test_1))\n",
    "\n",
    "x_test_2, y_test_2  = exclude_digits(x_test, y_test, excluded_digits=[2, 5, 8])\n",
    "print(\"y_test_2 : \",set(y_test_2))\n",
    "\n",
    "x_test_3, y_test_3  = exclude_digits(x_test, y_test, excluded_digits=[4,6,9])\n",
    "print(\"y_test_3 : \",set(y_test_3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = [(x_test_1, y_test_1),(x_test_2, y_test_2),(x_test_3, y_test_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model training\n",
    "- Sample model\n",
    "- training method\n",
    "- Evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,pth_x,pth_y):\n",
    "    batch_size = 64\n",
    "    epochs = 20\n",
    "    num_batches = len(pth_x) // batch_size\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    pth_y_onehot = tf.keras.utils.to_categorical(pth_y, num_classes=10)\n",
    "\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for i in range(num_batches):\n",
    "            # Get a batch of data\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            x_batch = pth_x[start:end]\n",
    "            y_batch = pth_y_onehot[start:end]\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(x_batch, training=True)  # Forward pass\n",
    "                loss = loss_fn(y_batch, predictions)        # Compute loss\n",
    "            \n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables) \n",
    "        \n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # Update weights\n",
    "\n",
    "            if i % 200 == 0:  # Print progress every 200 batches\n",
    "                print(f\"Batch {i}/{num_batches}, Loss: {loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,x_test,y_test):\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss = loss_fn(y_test_onehot, model(x_test))\n",
    "    print(\"test_loss : \",test_loss)\n",
    "    test_accuracy = tf.keras.metrics.categorical_accuracy(y_test_onehot, model(x_test))\n",
    "    print(\"test_accuracy : \",test_accuracy)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fed learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, net, trainset, testset):\n",
    "        self.net = net\n",
    "        self.trainset = trainset\n",
    "        self.testset = testset\n",
    "\n",
    "    # Train the model\n",
    "    def fit(self, parameters, config):\n",
    "        self.net.set_weights(parameters)\n",
    "        pth_x,pth_y = self.trainset # (x,y)\n",
    "        train_model(self.net,pth_x,pth_y )\n",
    "        return self.net.get_weights(), len(pth_y), {}\n",
    "\n",
    "    # Test the model\n",
    "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]):\n",
    "        self.net.set_weights(parameters)\n",
    "        loss, accuracy = evaluate_model(self.net, self.testset)\n",
    "        return loss, len(self.testset), {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client function\n",
    "def client_fn(context: Context) -> Client:\n",
    "    net = SimpleNN()\n",
    "    partition_id = int(context.node_config[\"partition-id\"])\n",
    "    client_train = train_sets[int(partition_id)]\n",
    "    client_test = test_sets[int(partition_id)]\n",
    "    return FlowerClient(net, client_train, client_test).to_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ClientApp(client_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss :  tf.Tensor(2.573329, shape=(), dtype=float32)\n",
      "test_accuracy :  tf.Tensor([0. 0. 1. ... 0. 0. 0.], shape=(6827,), dtype=float32)\n",
      "test accuracy on [1,3,7]:  tf.Tensor([0. 0. 1. ... 0. 0. 0.], shape=(6827,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# test code \n",
    "net = SimpleNN()\n",
    "_, accuracy137 = evaluate_model(net, x_test_1,y_test_1)\n",
    "print(\"test accuracy on [1,3,7]: \", accuracy137)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluate for model testing\n",
    "- The evaluate method evaluates the performance of the neural network model using the provided parameters and the test dataset (testset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(server_round, parameters, config):\n",
    "    net = SimpleNN()\n",
    "    net.set_weights(parameters)\n",
    "\n",
    "    _, accuracy137 = evaluate_model(net, x_test_1,y_test_1)\n",
    "    _, accuracy258 = evaluate_model(net, x_test_2,y_test_2)\n",
    "    _, accuracy469 = evaluate_model(net, x_test_3,y_test_3)\n",
    "\n",
    "    print(\"test accuracy on [1,3,7]: \", accuracy137)\n",
    "    print(\"test accuracy on [2,5,8]: \", accuracy258)\n",
    "    print(\"test accuracy on [4,6,9]: \", accuracy469)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The federated averaging strategy (`strategy.FedAvg`) is created for federated learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNN()\n",
    "params = ndarrays_to_parameters(net.get_weights())\n",
    "\n",
    "def server_fn(context: Context):\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.0,\n",
    "        initial_parameters=params,\n",
    "        evaluate_fn=evaluate,\n",
    "    )\n",
    "    config=ServerConfig(num_rounds=3)\n",
    "    return ServerAppComponents(\n",
    "        strategy=strategy,\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import ERROR\n",
    "backend_setup = {\"init_args\": {\"logging_level\": ERROR, \"log_to_driver\": False}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=3, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy on [1,3,7]:  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(6827,), dtype=float32)\n",
      "test accuracy on [2,5,8]:  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(7102,), dtype=float32)\n",
      "test accuracy on [4,6,9]:  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(7051,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Epoch 1/20\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Batch 0/638, Loss: 2.2316\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Epoch 2/20\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Batch 200/638, Loss: 0.0225\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Epoch 3/20\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Batch 400/638, Loss: 0.0916\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Epoch 4/20\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Batch 600/638, Loss: 0.0750\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Epoch 6/20\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Batch 0/668, Loss: 0.0103\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Epoch 7/20\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Batch 200/668, Loss: 0.0742\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Epoch 8/20\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Batch 600/638, Loss: 0.0179\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Epoch 10/20\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Batch 200/638, Loss: 0.0011\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Epoch 11/20\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Batch 400/638, Loss: 0.0035\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Epoch 12/20\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Batch 200/668, Loss: 0.0081\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Epoch 14/20\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Batch 400/668, Loss: 0.0137\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Epoch 15/20\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Batch 600/668, Loss: 0.0014\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Epoch 16/20\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Batch 600/638, Loss: 0.0013\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
      "\u001b[36m(ClientAppActor pid=25304)\u001b[0m Epoch 17/20\n",
      "\u001b[36m(ClientAppActor pid=25302)\u001b[0m Batch 600/668, Loss: 0.0005\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initiate the simulation passing the server and client apps\n",
    "# Specify the number of super nodes that will be selected on every round\n",
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=3,\n",
    "    #backend_config=backend_setup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
